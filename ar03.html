<!DOCTYPE html>
<html lang="zh-Hant">
<head>
  <meta charset="utf-8" />
  <title>Face AR (iPhone16 Pro + MediaPipe Tasks)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: black;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    }
    #debug {
      position: fixed;
      top: 0;
      left: 0;
      padding: 8px 12px;
      background: rgba(0, 0, 0, 0.5);
      color: #00ff00;
      font-size: 14px;
      z-index: 20;
    }
    #videoWrap {
      position: fixed;
      inset: 0;
      overflow: hidden;
      z-index: 0;
    }
    #video {
      width: 100%;
      height: 100%;
      object-fit: cover;
      transform: scaleX(-1); /* è‡ªæ‹é¡åƒ */
    }
    #overlay {
      position: fixed;
      inset: 0;
      z-index: 10;
      pointer-events: none;
    }
  </style>
</head>

<body>
  <div id="debug">åˆå§‹åŒ–ä¸­â€¦</div>

  <!-- ç›´æ¥é¡¯ç¤ºå‰é¡é ­ç•«é¢ -->
  <div id="videoWrap">
    <video id="video" playsinline autoplay muted></video>
  </div>

  <!-- åœ¨ä¸Šé¢ç•«è‡‰éƒ¨ç·šæ¢ -->
  <canvas id="overlay"></canvas>

  <!-- MediaPipe Tasks for Web -->
  <script type="module">
    import {
      FaceLandmarker,
      FilesetResolver,
      DrawingUtils,
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.15/vision_bundle.mjs";

    const debug = (msg) => (document.getElementById("debug").innerText = msg);

    const video = document.getElementById("video");
    const overlay = document.getElementById("overlay");
    const ctx = overlay.getContext("2d");

    function resizeCanvas() {
      overlay.width = window.innerWidth;
      overlay.height = window.innerHeight;
    }
    window.addEventListener("resize", resizeCanvas);

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: false,
        video: {
          facingMode: "user",
          width: { ideal: 1280 },
          height: { ideal: 720 },
        },
      });
      video.srcObject = stream;

      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          video.play();
          resizeCanvas();
          resolve();
        };
      });
    }

    let faceLandmarker;
    let running = false;

    async function setupFaceLandmarker() {
      debug("ç›¸æ©Ÿæ­£å¸¸ï¼Œè¼‰å…¥äººè‡‰æ¨¡å‹â€¦");

      const vision = await FilesetResolver.forVisionTasks(
        // é€™æ˜¯ tasks-vision çš„ wasm ä½ç½®ï¼ˆå®˜æ–¹å»ºè­°ï¼‰
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.15/wasm"
      );

      faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
        baseOptions: {
          // ğŸ”¥ é€™ä¸€è¡Œæ˜¯ã€Œæ­£ç¢ºçš„å®˜æ–¹æ¨¡å‹ç¶²å€ã€
          // æ³¨æ„è·¯å¾‘ï¼šmediapipe-models / face_landmarker / face_landmarker / float16 / 1 / face_landmarker.task
          modelAssetPath:
            "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
        },
        runningMode: "VIDEO",
        numFaces: 1,
      });

      debug("äººè‡‰æ¨¡å‹è¼‰å…¥å®Œæˆ âœ”");
    }

    function drawFaceLandmarks(result) {
      ctx.clearRect(0, 0, overlay.width, overlay.height);

      if (!result.faceLandmarks || !result.faceLandmarks[0]) {
        debug("ç›¸æ©Ÿæ­£å¸¸ï¼Œæ‰¾ä¸åˆ°è‡‰ï¼Œè«‹æŠŠè‡‰é è¿‘ä¸€é»");
        return;
      }

      debug("âœ” å·²åµæ¸¬åˆ°è‡‰");

      const landmarks = result.faceLandmarks[0];

      // ä»¥ã€Œå·¦çœ¼å¤–è§’(33)ã€åˆ°ã€Œå³çœ¼å¤–è§’(263)ã€ç•«ä¸€æ¢ç·šç•¶ç¤ºç¯„
      const leftEye = landmarks[33];
      const rightEye = landmarks[263];

      const x1 = leftEye.x * overlay.width;
      const y1 = leftEye.y * overlay.height;
      const x2 = rightEye.x * overlay.width;
      const y2 = rightEye.y * overlay.height;

      ctx.strokeStyle = "#00ff00";
      ctx.lineWidth = 5;
      ctx.beginPath();
      ctx.moveTo(x1, y1);
      ctx.lineTo(x2, y2);
      ctx.stroke();
    }

    async function startLoop() {
      if (!faceLandmarker) return;
      running = true;

      const loop = () => {
        if (!running) return;

        const nowMs = performance.now();
        const result = faceLandmarker.detectForVideo(video, nowMs);
        if (result) {
          drawFaceLandmarks(result);
        }

        requestAnimationFrame(loop);
      };
      loop();
    }

    (async () => {
      try {
        debug("è«‹å…è¨±ä½¿ç”¨ç›¸æ©Ÿâ€¦");
        await setupCamera();
        await setupFaceLandmarker();
        await startLoop();
      } catch (err) {
        console.error(err);
        debug("âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š" + err);
      }
    })();
  </script>
</body>
</html>